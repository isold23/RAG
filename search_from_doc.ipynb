{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isold23/RAG/blob/main/search_from_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python langchain faiss-gpu sentence-transformers==2.2.2 InstructorEmbedding \"unstructured[pdf]\""
      ],
      "metadata": {
        "id": "uS7y9d_EEU3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "Cu7BQ0Rxy-vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import (\n",
        "    StreamingStdOutCallbackHandler\n",
        ")\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.document_loaders import UnstructuredFileLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "print(\"sys.path[0] = \", sys.path[0])\n",
        "print(os.listdir(\"/content/drive/MyDrive\"))\n",
        "os.chdir(\"/content/drive/MyDrive\")\n",
        "print(\"sys.path[0] = \", sys.path[0])\n",
        "\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/drive/MyDrive/llama-2-7b-chat.ggmlv3.q2_K.bin\",\n",
        "    #model_path=\"/drive/MyDrive/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
        "    n_ctx=6000,\n",
        "    n_gpu_layers=512,\n",
        "    n_batch=30,\n",
        "    callback_manager=callback_manager,\n",
        "    temperature = 0.9,\n",
        "    max_tokens = 4095,\n",
        "    n_parts=1,\n",
        "\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "\n",
        "#loader = UnstructuredFileLoader(\"AI discovers over 20K taxable French swimming pools.docx\")\n",
        "loader = UnstructuredFileLoader(\"/content/drive/MyDrive/wuya.pdf\")\n",
        "\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# embedding engine\n",
        "hf_embedding = HuggingFaceInstructEmbeddings()\n",
        "\n",
        "db = FAISS.from_documents(docs, hf_embedding)\n",
        "\n",
        "# save embeddings in local directory\n",
        "db.save_local(\"faiss_AiArticle\")\n",
        "\n",
        "# load from local\n",
        "db = FAISS.load_local(\"faiss_AiArticle/\", embeddings=hf_embedding)\n",
        "\n",
        "#query = \"why french government using AI?\"\n",
        "query = \"用30字概括pdf文档内容\"\n",
        "search = db.similarity_search(query, k=2)\n",
        "print(\"---------------------\")\n",
        "print(search)\n",
        "\n",
        "template = '''Context: {context}\n",
        "\n",
        "Based on Context provide me answer for following question\n",
        "Question: {question}\n",
        "\n",
        "Tell me the information about the fact. The answer should be from context only\n",
        "do not use general knowledge to answer the query'''\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template= template)\n",
        "final_prompt = prompt.format(question=query, context=search)\n",
        "\n",
        "llm_chain.run(final_prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "OOclWf60fb1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}